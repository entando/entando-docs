= Cloud deployments on AWS, Azure and GCE
[id='cloud-deployments']
:sectnums:
:sectanchors:
:imagesdir: images/

== Overview

Entando currently offers a variety of Docker images that can be used in different container environments, such as
'vanilla' Docker, Openshift or Kubernetes. These images were also developed with different use cases in mind, such as
for simple demos, for getting started with your own app, for more advanced CI/CD, or for production deployments.
In this chapter we are mainly interested in showing you how to use those images in a Kubernetes environment hosted somewhere
in some datacenter of your favorite hosting cloud provider.

== Assumptions

We assume that you have already a kubernetes cluster up and running (also a minikube running on your local machine), that
 you know how to write a yaml file for kubernetes and that you already have your docker images
 (http://dev.entando.org/#_docker_overview) published on your docker registry and of course you have kubectl installed
  (https://kubernetes.io/docs/tasks/tools/install-kubectl/[how to install kubectl]).

[[getting-started-k8s]]
== Getting started

[[simplest-example]]
=== The Simplest example

Let's start with a very simple all in one docker image that contains everything needed to run an entando project:

- *A java container* (jetty)
- *2 DBs* (Derby)

This is the Dockerfile that we are going to use for this example:

[source,dockerfile,indent=0]
----
FROM entando/centos-base:master
USER entando

ENV PROJECT_HOME /home/entando
ENV PROJECT_NAME k8s-simplest

WORKDIR ${PROJECT_HOME}

RUN mvn archetype:generate -B -Dfilter=entando -DarchetypeGroupId=org.entando.entando -DarchetypeArtifactId=entando-archetype-webapp-generic -DgroupId=org.entando -DartifactId=${PROJECT_NAME} -Dversion=1.0-SNAPSHOT -Dpackage=test.entando

WORKDIR ${PROJECT_HOME}/${PROJECT_NAME}

CMD ["mvn", "jetty:run", "-Dorg.eclipse.jetty.annotations.maxWait=180"]

EXPOSE 8080
----

With this in place we are going to build our image and push it on our docker registry. This registry** has to be reachable from our kubernetes**
cluster.

`docker build -t myregistry/k8s-simplest:test .`

`docker push myregistry/k8s-simplest:test`

The next step is to create a *deployment* configuration file for kubernetes:

[source,yaml,indent=0]
----
# Deployment configuration definition for k8s-simplest
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-simplest-deploy
spec:
  selector:
    matchLabels:
      run: k8s-simplest
      app: k8s-simplest-demo
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        run: k8s-simplest
        app: k8s-simplest-demo
    spec:
      containers:
        - name: k8s-simplest
          image: myregistry/k8s-simplest:test
          resources:
            requests:
              memory: 2048Mi
              cpu: 0.50
            limits:
              memory: 2Gi
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
...
# Service definition for the k8s-simplest app
---
apiVersion: v1
kind: Service
metadata:
  name: k8s-simplest-service
  labels:
    name: k8s-simplest-service
    app:  k8s-simplest-demo
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
  selector:
    run: k8s-simplest
    app: k8s-simplest-demo
...
----

Save the content of this yaml file on your path (i.e k8s-simplest.yml) and execute:

`kubectl create -f k8s-simplest.yml`

This file also created a LoadBalancer to expose our entando project on port 80. That's it.

We have just publish our first container on a kubernetes cluster.

[[n-tier-example]]
=== An n-tier example

Let's continue from our k8s-simplest project. For this demo we are going to use:

- Wildfly 15.0.1.Final release
- Postgresql 9.6 release
- Entando 5.0.3 release (our k8s-simplest project)
- Infinispan 9.4.8.Final release

`Please remember that the following instructions are not meant to be used on production environments but are only samples
on how to deploy entando on a kubernetes cluster.`

[[postgresql-96-k8s]]
==== Postgresql

The first image that we need to deploy is the DB server. For the sake of this demonstration
we have already prepared a docker image that contains the two DBs (k8s-simplestPort, k8s-simplestServ) so the only
thing that we need now is the deployment configuration file for kubernetes:

[source,yaml,indent=0]
----
---
apiVersion: v1
kind: Pod
metadata:
  name: postgres96-pod
  labels:
    name: postgres96-pod
    app:  k8s-demo
spec:
  containers:
    - name: postgres96
      image: entando/postgres-96:k8s-simplest
      ports:
        - containerPort: 5432
          protocol: TCP
      resources:
        requests:
          cpu: 0.50
...
# Service
---
apiVersion: v1
kind: Service
metadata:
  name: postgres96
  labels:
    name: postgres96-service
    app: k8s-demo
spec:
  ports:
    - port: 5432
      targetPort: 5432
  selector:
    name: postgres96-pod
    app:  k8s-demo
...
----

We have exposed the DB server on port 5432 and named it `postgres96-service`.

[[infinispan-cluster-k8s]]
==== Infinispan cluster

The second docker image that we are going to deploy on our kubernetes cluster is the infinispan's one but to be able to
make this image works correctly first we need to set correct permissions to the service account and assign the view role
so that the jgroups subsystem is able to form a cluster.
We are going to execute the following command:

**Infinispan permissions**

[source,bash,indent=0]
----
kubectl create rolebinding infinispan \
  --clusterrole=view \
  --user=default \
  --namespace=default \
  --group=system:serviceaccounts
----

Now we can go on and set the deployment configuration file:

[source,yaml,indent=0]
----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
  labels:
    application: infinispan-server
  name: infinispan-server
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      application: infinispan-server
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        application: infinispan-server
        deploymentConfig: infinispan-server
    spec:
      containers:
        - args:
            - cloud
            - -Djboss.default.jgroups.stack=kubernetes
          env:
            - name: KUBERNETES_LABELS
              value: application=infinispan-server
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MGMT_USER
              value: "demo"
            - name: MGMT_PASS
              value: "demo"
            - name: APP_USER
              value: "demo"
            - name: APP_PASS
              value: "demo"
          image: entando/infinispan-server:k8s-simplest
          imagePullPolicy: Always
          name: infinispan-server
          ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8181
              protocol: TCP
            - containerPort: 8888
              protocol: TCP
            - containerPort: 9990
              protocol: TCP
            - containerPort: 11211
              protocol: TCP
            - containerPort: 11222
              protocol: TCP
            - containerPort: 11223
              protocol: TCP
          resources:
            requests:
              memory: 2Gi
              cpu: 0.50
            limits:
              memory: 4Gi
          terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
...
# Internal service configuration
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    description:  Hot Rod's port.
  name: infinispan-service
  labels:
    application: infinispan-server
    app: k8s-demo
spec:
  ports:
    - port: 11222
      targetPort: 11222
      name: hotrod
      protocol: TCP
    - port: 11223
      targetPort: 11223
      name: hotrod-internal
      protocol: TCP
  selector:
    deploymentConfig: infinispan-server
...
----

The configuration file is self explanatory just have a look at the containers args that we are using (we have set the
default jgroups stack as kubernetes).

For this deployment the service (infinispan-service) exposes port 11222 and 11223 that we are going to use to populate and
retreive entries for/from the entando's caches by the hotrod protocol.

[[wildfly-15-01-final-k8s]]
==== Wildfly-15.0.1.Final

Finally we need the deployment configuration file for our wildfly docker image to deploy our k8s-simplest war. As for the
infinispan docker image we need view permissions to be able to make the jgroups protocol to see other pods and form the cluster:

[source,bash,indent=0]
----
kubectl create rolebinding default-viewer \
  --clusterrole=view \
  --serviceaccount=default:default \
  --namespace=default
----

After that we are going to create the deployment configuration file and the service definition that this time will be a
load balancer exposing the port 80 and mapping it to port 8080 of our wildfly pods.

[source,yaml,indent=0]
----
# Deployment configuration definition for wildfly15-01-ha
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-demo-deploy
spec:
  selector:
    matchLabels:
      run: wildfly15-01-ha
      app: k8s-demo
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        run: wildfly15-01-ha
        app: k8s-demo
    spec:
      containers:
        - name: wildfly15-01-ha
          image: entando/wildlfy-15.0.1.final:k8s-simplest
          resources:
            requests:
              memory: 2048Mi
              cpu: 0.50
            limits:
              memory: 4Gi
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
            - containerPort: 7600 #jgroups default
            - containerPort: 8888 #jgroups undertow
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: PG_ENTANDO_PORT_DB_JNDI_NAME
              value: "java:jboss/datasources/k8s-simplestPortDataSource"
            - name: PG_ENTANDO_SERV_DB_JNDI_NAME
              value: "java:jboss/datasources/k8s-simplestServDataSource"
            - name: PG_ENTANDO_PORT_DB_CONNECTION_STRING
              value: "postgres96:5432/k8s-simplestPort"
            - name: PG_ENTANDO_SERV_DB_CONNECTION_STRING
              value: "postgres96:5432/k8s-simplestServ"
            - name: PG_USERNAME
              value: "agile"
            - name: PG_PASSWORD
              value: "agile"
            - name: INITIAL_POOL_SIZE
              value: "5"
            - name: MAX_POOL_SIZE
              value: "10"
...
# Service definition for the wildfly15-01-ha
---
apiVersion: v1
kind: Service
metadata:
  name: wildfly15-01-ha-service
  labels:
    name: wildfly15-01-ha-service
    app:  k8s-demo
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
  selector:
    run: wildfly15-01-ha
    app: k8s-demo
...
----

We have passed as ENV vars to this deployment configuration the JNDI name, the DB's connection string tha is linked to our
internal service name (postgres96). All the othe options are self explanatory.

[[entando-project-configuration-k8s]]
==== Entanto project configuration

To be able to use the remote infinispan server cluster we have modified the entando project to use the hotrod protocol for
all the tasks related to caching and to make the overall deployment more scalabale and
container (Java container) indipendent.

To see how the entando project has been modified to work with the datagrid implementation take a loook at the code on
github (https://github.com/entando/k8s-simplest/tree/kubernetes[kubernetes branch])

Here is the client hotrod configuration file:

[source,ini,indent=0]
----
infinispan.client.hotrod.server_list=infinispan-service:11222
infinispan.client.hotrod.marshaller=org.infinispan.commons.marshall.jboss.GenericJBossMarshaller
infinispan.client.hotrod.async_executor_factory=org.infinispan.client.hotrod.impl.async.DefaultAsyncExecutorFactory
infinispan.client.hotrod.request_balancing_strategy=org.infinispan.client.hotrod.impl.transport.tcp.RoundRobinBalancingStrategy
infinispan.client.hotrod.tcp_no_delay=true
-infinispan.client.hotrod.key_size_estimate=128
infinispan.client.hotrod.value_size_estimate=1024
infinispan.client.hotrod.force_return_values=false
infinispan.client.hotrod.client_intelligence=HASH_DISTRIBUTION_AWARE
infinispan.client.hotrod.batch_Size=10000

# authentication
infinispan.client.hotrod.use_auth=true
infinispan.client.hotrod.sasl_mechanism=DIGEST-MD5
infinispan.client.hotrod.auth_username=demo
infinispan.client.hotrod.auth_password=demo
infinispan.client.hotrod.auth_realm=ApplicationRealm
infinispan.client.hotrod.auth_server_name=infinispan-service

## near cache
infinispan.client.hotrod.near_cache.mode=INVALIDATED
infinispan.client.hotrod.near_cache.max_entries=-1
infinispan.client.hotrod.near_cache.name_pattern=Entando_*

## below is connection pooling config
infinispan.client.hotrod.connection_pool.max_active=-1
infinispan.client.hotrod.connection_pool.exhausted_action=CREATE_NEW
infinispan.client.hotrod.connection_pool.min_evictable_idle_time=1800000
infinispan.client.hotrod.connection_pool.min_idle=1
----

To deploy the project on kubernetes just follow this order:

- Deploy the DB server (*kubectl create -f postgres96.yml*)
- Assign correct permissions to default user for infinispan and wildfly cluster
- Deploy the infinispan cluster (*kubectl create -f infinispan.yml*)
- Deploy the wildfly cluster (*kubectl create -f wildfly.yml*)

**Known Issues**

At this point this demo will not work correctly due to a missing jar required on wildfly. This will be patched in a new
Entando release.
